{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__\n",
    "\n",
    "# the debate : https://blog.ekbana.com/private-nltk-vs-spacy-3926b3674ee4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.46.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.2.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "symbolic link created for C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\spacy\\data\\en <<===>> C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\en_core_web_sm\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[+] Linking successful\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\en_core_web_sm\n",
      "-->\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\spacy\\data\\en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "# POS ( part of speech tagging)Part-of-speech (POS) tagging is a process where you read some text and assign parts of\n",
    "#speech to each word or token, such as noun, verb, adjective, etc.\n",
    "#POS tagging becomes extremely important when you want to identify some entity in\n",
    "#a given sentence. The first step is to do POS tagging and see what our text contains.\n",
    "\n",
    "# load the spacy modules \n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let VERB\n",
      "us PRON\n",
      "build VERB\n",
      "the DET\n",
      "chatbot NOUN\n",
      "and CCONJ\n",
      "use VERB\n",
      "it PRON\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en') # loads spacy en module into python\n",
    "doc = nlp('Let us build the chatbot and use it' ) # creating a doc object\n",
    "# more on 'doc' here : https://spacy.io/api/doc\n",
    "for token in doc:\n",
    "    print(token.text , token.pos_) # prints the text and then the p-o-s\n",
    "# why use '_' ? :: spaCy encodes all strings to hash values to reduce memory usage and improve efficiency. So to get the readable string representation of an attribute, we need to add an underscore _ to its name:\n",
    "\n",
    "# documentation : https://spacy.io/usage/linguistic-features\n",
    "\n",
    "\n",
    "#------------\n",
    "\n",
    "# printing tokens from doc object from method nlp which is continer for accessing the\n",
    "# annotations and we get POS tagged for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DET\n",
      "is AUX\n",
      "another DET\n",
      "try NOUN\n"
     ]
    }
   ],
   "source": [
    "# another example \n",
    "\n",
    "doc = nlp('This is another try')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This DET this DT Xxxx True True\n",
      "is AUX be VBZ xx True True\n",
      "same ADJ same JJ xxxx True True\n",
      "as SCONJ as IN xx True True\n",
      "last ADJ last JJ xxxx True True\n",
      "message NOUN message NN xxxx True False\n"
     ]
    }
   ],
   "source": [
    "# now a biiger version of example with many features of lingusitics \n",
    "\n",
    "doc = nlp('This is same as last message')\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.lemma_,token.tag_,token.shape_,token.is_alpha,token.is_stop)\n",
    "    \n",
    "#-------\n",
    "\n",
    "# lemma is root form of the word  , reading becomes read.\n",
    "# pos is part of speech of word\n",
    "# TAG is verb and it can be past present or future tense\n",
    "# shape is shape of word : capitals , digits , letters\n",
    "\n",
    "# https://spacy.io/api/token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\lang\\en\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f756c7306d9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLEMMA_INDEX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mLEMMA_RULES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEMMA_INDEX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLEMMA_EXC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLEMMA_RULES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# here noun is part of speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LEMMA_INDEX' from 'spacy.lang.en' (c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\lang\\en\\__init__.py)"
     ]
    }
   ],
   "source": [
    "# use of lemmatization\n",
    "\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX , LEMMA_EXC ,LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX,LEMMA_EXC,LEMMA_RULES)\n",
    "lemmatizer('cat','NOUN') # here noun is part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LEMMA_EXC' from 'spacy.lang.en' (c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\lang\\en\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f414b01f17fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0men\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mLEMMA_EXC\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mLEMMA_RULES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLEMMA_EXC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mLEMMA_RULES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'NOUN'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# here noun is part of speech\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LEMMA_EXC' from 'spacy.lang.en' (c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\spacy\\lang\\en\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import  LEMMA_EXC ,LEMMA_RULES\n",
    "lemmatizer = Lemmatizer(LEMMA_EXC,LEMMA_RULES)\n",
    "lemmatizer('cat','NOUN') # here noun is part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from spacy.lang.en import English\\nlemmatizer = English.Defaults.create_lemmatizer()\\nfrom spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is fixed in v2.2\n",
    "#https://stackoverflow.com/questions/58779371/importerror-cannot-import-name-lemma-index-from-spacy-lang-en\n",
    "\n",
    "# The lemmatizer data is now stored in the separate package spacy-lookups-data and the Lemmatizer is initialized with a Lookups object instead of the individual v\n",
    "# now Im installing spacy lookups data.\n",
    "\n",
    "\"\"\"from spacy.lang.en import English\n",
    "lemmatizer = English.Defaults.create_lemmatizer()\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\"\"\"\n",
    "\n",
    "\n",
    "#from spacy.lemmatizer import Lemmatizer\n",
    "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "#lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "#lemmatizer('chuckles', 'NOUN') \n",
    "\n",
    "\n",
    "# so this is not working as of now. So moving on for now.\n",
    "# will come back later at this.\n",
    "\n",
    "# will try an alternative : Lefff Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 million dollars MONEY\n"
     ]
    }
   ],
   "source": [
    "# examples of NER ( named entity recognition)\n",
    "\n",
    "my_string = ('I want to buy a plane of 0.1 million dollars')\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ORG\n",
      "0.1 million dollars MONEY\n",
      "Boeing ORG\n",
      "Germany GPE\n"
     ]
    }
   ],
   "source": [
    "my_string = ('Google want to buy a plane of 0.1 million dollars from Boeing in Germany')\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "\n",
    "    \n",
    "# ORG is for organization i.e comapany name\n",
    "# money is money..\n",
    "# GPE is geo political entity i.e name of country\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bill PERSON\n",
      "90% PERCENT\n",
      "100 million dollars MONEY\n",
      "age of 60 DATE\n"
     ]
    }
   ],
   "source": [
    "# another one\n",
    "\n",
    "my_string = (' Bill gates wants to donate 90% of 100 million dollars money to me at age of 60 in year 2020')\n",
    "doc = nlp(my_string)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)\n",
    "    \n",
    "# spacy supports NER as per ontonotes 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'still', '’s', 'no', 'when', 'any', 'nowhere', 'call', 'them', 'where', 'something', 'top', 'twelve', 'unless', 'among', 'twenty', 'until', 'how', 'go', 'every', 'ever', 'afterwards', 'thereby', 'without', 'themselves', 'not', 'within', 'made', 'various', 'under', 'off', 'ca', 'mine', 'take', 'put', 'myself', '’ll', 'part', 'seem', 'whole', 'thence', 'thereupon', 'onto', 'whereas', 'n’t', 'former', 'some', 'used', 'by', 'up', 'itself', 'him', 'whenever', 'rather', 'can', 'yours', 'whoever', 'else', 'or', 'that', 'had', 'thereafter', '‘m', 'out', 'two', 'been', 'very', 'with', 'toward', 'why', 'either', 'may', 'herein', 'before', 'since', 'nine', 'while', '‘re', 'thus', '‘d', 'anything', 'which', 'namely', 'ours', 'full', 'whether', 'in', 'everything', 'ourselves', 'latterly', 'their', 'towards', 'moreover', 'did', 'he', 'five', 'hereupon', 'quite', 'say', 'becoming', 'seemed', 'me', 'give', 'from', 'thru', 'but', 'forty', 'anywhere', 'per', 'enough', 'side', 'therefore', 'whereafter', 'whither', 'whom', 'alone', 'upon', 'due', 'other', 'formerly', 'show', 'except', 'others', 'beyond', 'yourself', 'ten', '‘ll', 'many', '’ve', 'already', 'she', 'neither', 'therein', 'herself', 'there', 'everywhere', 'around', 'both', 'whereupon', 'mostly', 'these', 'bottom', 'nobody', 'fifty', 'because', \"'ll\", 'they', 'hundred', 'i', 'anyone', 'regarding', 'you', 'then', 'once', '’d', 'so', 'your', 'hereafter', '’re', 'above', 'sometime', 'back', 'all', 'more', 'using', 'must', 'after', 'at', 'somewhere', 'for', 'my', 'doing', '‘ve', 'is', 'however', '‘s', 'against', 'besides', 'several', 'three', 'below', 'could', 'almost', \"n't\", 'it', 'move', \"'m\", 'name', 'if', 'amount', 'we', 'would', 'really', 'though', 'nor', 'does', 'everyone', 'serious', 'us', 'be', 'sometimes', 'via', 'front', 'even', 'only', 'on', 'third', 'into', 'hereby', 'elsewhere', \"'re\", 'somehow', 'make', 'someone', 'whatever', \"'s\", 'nevertheless', 'keep', 'cannot', 'between', 'have', 'am', 'yet', 'four', 'indeed', 'beforehand', 'might', 'here', 'hence', 'the', 'anyway', 'down', 'hers', 'whose', 'yourselves', 'done', 'same', 'one', 'eight', 'as', 'eleven', 'will', 'throughout', 'becomes', 'those', 'anyhow', 'during', 'just', 'too', 'six', 'her', 'always', 'much', 'who', 'and', 'first', 'was', 'often', 'well', 'were', 'become', 'being', 'his', 'beside', 'of', 'has', 'wherein', 'whence', 'latter', \"'d\", 'across', 'himself', 'what', 'n‘t', 'an', 'than', 'such', 'over', 'fifteen', 'last', 'now', 'our', 'meanwhile', 'behind', 'should', 'next', 'never', \"'ve\", 'each', 'seems', 'amongst', 'most', '’m', 're', 'sixty', 'wherever', 'least', 'along', 'nothing', 'although', 'please', 'through', 'own', 'its', 'about', 'together', 'get', 'noone', 'another', 'see', 'few', 'became', 'perhaps', 'do', 'a', 'less', 'empty', 'otherwise', 'are', 'again', 'further', 'none', 'to', 'whereby', 'also', 'seeming', 'this'}\n"
     ]
    }
   ],
   "source": [
    "# To see all the words defined as stop words in spaCy\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check if a word is stop word\n",
    "#Stop words are a very important part of text clean up\n",
    "\n",
    "nlp.vocab['haha'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['is'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dependedncy parser \n",
    "\n",
    "# example : Book me a flight from Bangalore to Goa\n",
    "\n",
    "doc = nlp('Book me a flight from Bangalore to Goa')\n",
    "blr,goa = doc[5],doc[7]\n",
    "list(blr.ancestors)\n",
    "\n",
    "# here it understood that I need to book flight from banglore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[to, flight, Book]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(goa.ancestors)\n",
    "\n",
    "# here it understood that I want to book flight to goa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[from, flight, Book]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to list ancestors \n",
    "\n",
    "list(doc[5].ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking of table belongs to restaurant\n"
     ]
    }
   ],
   "source": [
    "# complex real life scenario \n",
    "\n",
    "# sentence is : I want to book a cab to the hotel and a table at a restaurant.\n",
    "\n",
    "doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "tasks = doc[2],doc[8] # this is for table and taxi.\n",
    "tasks_target = doc[5],doc[11] # for restaurant and hotel\n",
    "\n",
    "for task in tasks_target:\n",
    "    for tok in task.ancestors:\n",
    "        if tok in tasks:\n",
    "            print(\"Booking of {} belongs to {}\".format(tok,task))\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[a, at]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(doc[2].children) # children dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py:194: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"1975afc2d77c483f97104ad2293ebf65-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Book</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">table</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">restaurant</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">taxi</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">hotel</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-0\" stroke-width=\"2px\" d=\"M245,352.0 C245,264.5 385.0,264.5 385.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-1\" stroke-width=\"2px\" d=\"M70,352.0 C70,177.0 390.0,177.0 390.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390.0,354.0 L398.0,342.0 382.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M560.0,354.0 L568.0,342.0 552.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-5\" stroke-width=\"2px\" d=\"M945,352.0 C945,264.5 1085.0,264.5 1085.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1085.0,354.0 L1093.0,342.0 1077.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-7\" stroke-width=\"2px\" d=\"M945,352.0 C945,89.5 1445.0,89.5 1445.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,354.0 L1453.0,342.0 1437.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-8\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 1625.0,2.0 1625.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1625.0,354.0 L1633.0,342.0 1617.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-1975afc2d77c483f97104ad2293ebf65-0-10\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-1975afc2d77c483f97104ad2293ebf65-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1965.0,354.0 L1973.0,342.0 1957.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'dep' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [01/Jun/2020 02:16:29] \"GET / HTTP/1.1\" 200 10066\n",
      "127.0.0.1 - - [01/Jun/2020 02:16:29] \"GET /favicon.ico HTTP/1.1\" 200 10066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "# we can generate teh digarm for this.\n",
    "#spaCy v2.0+ has a visualization module where we can pass a Doc or a list of Doc\n",
    "#objects to displaCy and call serve method of displaCy to run the web server.\n",
    "\n",
    "\n",
    "from spacy import displacy\n",
    "doc = nlp('Book a table at the restaurant and the taxi to the hotel')\n",
    "displacy.serve(doc , style = 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User is refer Pune to visit\n",
      "User is refer Mysore to stay\n"
     ]
    }
   ],
   "source": [
    "# a complex scenario\n",
    "\n",
    "doc = nlp('What are places to visit in Pune and stay in Mysore')\n",
    "# here visit pune and stay in mysore\n",
    "\n",
    "places = [doc[6],doc[10]] # pune and mysore\n",
    "actions = [doc[4],doc[8]]  # visit and stay\n",
    "\n",
    "for place in places:\n",
    "    for tok in place.ancestors:\n",
    "        if tok in actions:\n",
    "            print(\"User is refer {} to {}\".format(place,tok))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Building chatbots, python, natural language processing area]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Noun chunks \n",
    "\n",
    "doc = nlp('Building chatbots with python is good for natural language processing area')\n",
    "list(doc.noun_chunks)\n",
    "#for chunk in noun_chunks:\n",
    "    #print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building chatbots\n",
      "python\n",
      "natural language processing area\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Building chatbots with python is good for natural language processing area')\n",
    "list(doc.noun_chunks)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "with\n",
      "for\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('Building chatbots with python is good for natural language processing area')\n",
    "list(doc.noun_chunks)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What [-2.6713011  -0.06569481  0.15939042 -0.96043694]\n",
      "is [-1.7289232  3.68001   -3.0862994 -2.1377673]\n",
      "the [-3.803561   -1.6094736   0.2145581  -0.98742336]\n",
      "time [-1.4022667  2.0931258 -2.269023   3.2789178]\n",
      "now [ 4.380091   -0.57312167  4.30987    -0.9169006 ]\n",
      "? [-1.1632923  0.2830859  1.31548    0.0414753]\n"
     ]
    }
   ],
   "source": [
    "# Finding Similarity\n",
    "\n",
    "# use of glo ve algorithm : http://text2vec.org/glove.html\n",
    "\n",
    "doc = nlp('What is the time now?')\n",
    "for token in doc:\n",
    "    print(token.text , token.vector[:4])\n",
    "    \n",
    "# using token.vector I converted it into vector form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py:194: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46760062447446527"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as an example :\n",
    "\n",
    "my_string1 = nlp('I like football')\n",
    "my_string2 = nlp('Ronaldo plays football')\n",
    "my_string1.similarity(my_string2)\n",
    "\n",
    "# so there is 46% similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\admin\\appdata\\local\\programs\\python\\python38\\lib\\runpy.py:194: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  return _run_code(code, main_globals, None,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6351398429661087"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example\n",
    "\n",
    "my_string1 = nlp('I like football')\n",
    "my_string2 = nlp('I play football a lot')\n",
    "my_string1.similarity(my_string2)\n",
    "\n",
    "# now the similarity is 63%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
